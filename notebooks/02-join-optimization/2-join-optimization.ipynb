{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7be49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "# Set environment variables (local paths)\n",
    "os.environ[\"JAVA_HOME\"] = \"D:/Programs/Java\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"D:/Programs/hadoop\"\n",
    "os.environ[\"SPARK_HOME\"] = \"D:/Programs/spark/spark-3.5.6-bin-hadoop3\"  # Adjust if different\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"D:/Programs/spark/spark-3.5.6-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d43c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Testing Partitioning\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8efa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_file = \"../../data/transactions.parquet\"\n",
    "customers_file = \"../../data/customers.parquet\"\n",
    "\n",
    "df_transactions = spark.read.parquet(transactions_file)\n",
    "df_customers = spark.read.parquet(customers_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613a054",
   "metadata": {},
   "source": [
    "| Join Type       | Broadcast Enabled | AQE Enabled | Expected Join               |\n",
    "| --------------- | ------------------| ----------- | --------------------------- |\n",
    "| Sort-Merge Join | âŒ (disabled)      | âŒ           | Sort-Merge                  |\n",
    "| Broadcast Join  | âœ… (enabled)       | âŒ           | Broadcast                   |\n",
    "| AQE Join        | âœ… (enabled)       | âœ…           | Adaptive (likely broadcast) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc4e6a",
   "metadata": {},
   "source": [
    "### Sort Merge Join vs Broadcast Join\n",
    "\n",
    "| Metric | **Sort Merge Join** | **Broadcast Hash Join** |\n",
    "| --- | --- | --- |\n",
    "| **Join Type Used** | SortMergeJoin | BroadcastHashJoin |\n",
    "| **Broadcast Table Size** | N/A | 32.2 MiB |\n",
    "| **Broadcast Build Time** | N/A | 218 ms |\n",
    "| **Broadcast Collect Time** | N/A | 3.8 s |\n",
    "| **Time to Broadcast** | N/A | 41 ms |\n",
    "| **Input Data Size** | 870.7 MiB (Parquet source) | 870.7 MiB (Parquet source) |\n",
    "| **Shuffle Read (Total)** | 2.4 GiB | 0 B (Broadcast avoids shuffling) |\n",
    "| **Shuffle Write (Total)** | 2.4 GiB | 0 B |\n",
    "| **Spill (Memory / Disk)** | 512 MiB / 140.5 MiB | 0 / 0 |\n",
    "| **Max Shuffle Read (per task)** | 1.1 GiB | 0 |\n",
    "| **Execution Time (Stage)** | ~5.1 min (Stage 4 alone was 3.3 min) | 1.1 min |\n",
    "| **# of Output Rows (Join Output)** | 39,790,092 | 39,790,092 |\n",
    "| **Number of Partitions in Join** | 200 | 15 (only from left side) |\n",
    "| **Skew Detected** | Yes (Shuffle Read: 1.1 GiB max for one task) | No skew (no shuffling) |\n",
    "\n",
    "#### **Key Observations and Inferences**\n",
    "\n",
    "1. **Broadcast Join avoids shuffling**:\n",
    "    - No shuffle read/write â†’ no skew â†’ less memory/disk spill.\n",
    "    - All data from `df_customers` was broadcast (only 32.2 MiB).\n",
    "2. **Sort Merge Join is expensive**:\n",
    "    - High shuffle read/write â†’ ~2.4 GiB in both directions.\n",
    "    - Significant skew (some tasks reading 1.1 GiB).\n",
    "    - Required sorting on both sides + high memory/disk usage.\n",
    "    - Worst task took 3.1 minutes.\n",
    "3. **Broadcast Join is more efficient for small dimension tables**:\n",
    "    - Avoids shuffle.\n",
    "    - Reduces execution time.\n",
    "    - Reduces memory pressure and GC time.\n",
    "\n",
    "#### Why Only 15 Tasks in Broadcast Join?\n",
    "\n",
    "Because only the **left side** (fact table) was partitioned and parallelized. The right side (`df_customers`) was broadcast and handled by the driver. So execution happens only across partitions of the left table (`df_transactions`, which probably has 15 partitions after your latest transformation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7338f4",
   "metadata": {},
   "source": [
    "## 1. Sort-Merge Join Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5de3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)  # Disable broadcast join\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")       # Disable AQE\n",
    "df_joined = df_transactions.join(df_customers, on=\"cust_id\", how=\"inner\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save(\"output/sort_merge_join\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75081522",
   "metadata": {},
   "source": [
    "| Metric | Value |\n",
    "| --- | --- |\n",
    "| **Join Type Used** | SortMergeJoin |\n",
    "| **Number of Output Rows** | 39,790,092 |\n",
    "| **Total Execution Time** | 5.1 min |\n",
    "| **Input Size (Combined)** | 870.7 MiB |\n",
    "| **# of Tasks (Completed)** | 215 |\n",
    "| **# of Partitions in Join** | 200 |\n",
    "\n",
    "**First Exchange (Transactions Table)**\n",
    "| Metric | Value |\n",
    "| --- | --- |\n",
    "| **Shuffle Write (MB)** | 2.4 GiB (~2457 MB) |\n",
    "| **Shuffle Read (MB)** | 2.4 GiB (~2457 MB) |\n",
    "| **# of Records Written** | 39,790,092 |\n",
    "| **Peak Memory Used (Sort)** | 11.5 GiB (max 3.9 GiB per task) |\n",
    "| **Spill Size (Sort)** | 512 MiB |\n",
    "\n",
    "**Second Exchange (Customers Table)**\n",
    "| Metric | Value |\n",
    "| --- | --- |\n",
    "| **Shuffle Write (MB)** | 383.8 KiB (~0.375 MB) |\n",
    "| **Shuffle Read (MB)** | 712.3 KiB (~0.695 MB) |\n",
    "| **# of Records Written** | 5,000 |\n",
    "| **Peak Memory Used (Sort)** | 6.3 GiB |\n",
    "| **Spill Size (Sort)** | 0.0 B |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162133a",
   "metadata": {},
   "source": [
    "### Inference & Observations\n",
    "\n",
    "1. **Why high shuffle size for Sort-Merge Join?**\n",
    "    - SMJ requires both sides of the join to be **sorted** and **shuffled by join key** (`cust_id`). This causes **full data shuffles** on both sides (especially the large `transactions` DataFrame).\n",
    "2. **Why two exchanges?**\n",
    "    - One `Exchange` for sorting and shuffling `transactions`.\n",
    "    - One `Exchange` for sorting and shuffling `customers`.\n",
    "3. **Memory Pressure:**\n",
    "    - One of the sort stages spilled to disk (512 MB) â€” indicates **insufficient memory for sorting** some partitions.\n",
    "4. **Join Output:**\n",
    "    - You got ~40 million joined rows. Despite smaller `customers` table, Spark didnâ€™t use Broadcast Join because auto broadcast was **disabled**.\n",
    "5. **Total execution time is high** due to:\n",
    "    - Large shuffle read/write (especially 2.4 GiB).\n",
    "    - Sorting overhead (spill to disk).\n",
    "    - 200 tasks running under memory pressure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76736955",
   "metadata": {},
   "source": [
    "#### **Q1: Why was Shuffle Read 2.4â€¯GiB if the data was only ~800â€¯MB?**\n",
    "\n",
    "> Because the same data is read and shuffled across multiple partitions and tasks â€” not just once.\n",
    "> \n",
    "- You **repartitioned** your data into **200 partitions** (we see `number of partitions: 200`).\n",
    "- Your 800â€¯MB dataset is **shuffled and split** into 200 parts.\n",
    "- Every task in a downstream stage **may read shuffle blocks from multiple upstream partitions** (i.e., data is *fanned out*).\n",
    "- This means **some shuffle blocks get copied multiple times**, and **shuffle read includes all bytes read across all tasks**, not just unique bytes.\n",
    "- So:\n",
    "    \n",
    "    ðŸ‘‰ Actual dataset = 800â€¯MB\n",
    "    \n",
    "    ðŸ‘‰ Shuffle write (unique data shuffled) = 2.4â€¯GiB\n",
    "    \n",
    "    ðŸ‘‰ Shuffle read (total across all tasks) = 2.4â€¯GiB\n",
    "    \n",
    "- It includes:\n",
    "    - Multiple tasks reading overlapping data\n",
    "    - Possible internal serialization and deserialization overheads\n",
    "    - Metadata, small file overhead, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4de74",
   "metadata": {},
   "source": [
    "#### Evidence of Skew from the Metrics\n",
    "\n",
    "| Metric | Median | Max | Skew Indicator? |\n",
    "| --- | --- | --- | --- |\n",
    "| **Duration** | 0.7 s | **3.1 min** | âœ… Yes (very high max) |\n",
    "| **Shuffle Read Size** | 6.7 MiB | **1.1 GiB** | âœ… Yes (huge diff) |\n",
    "| **Shuffle Read Records** | 110,390 | **~1.76 million** | âœ… Yes (16x higher) |\n",
    "| **Spill (disk)** | 0 | **140.5 MiB** | âœ… Yes (spill only on 1 task) |\n",
    "    \n",
    "- **Root Cause in Your Case**\n",
    "    \n",
    "    Based on the physical plan and exchange metrics, this likely happened during the **SortMergeJoin**:\n",
    "    \n",
    "    - You had **200 partitions**.\n",
    "    - But **some keys (like a `cust_id`) occurred way more frequently** than others.\n",
    "    - That **caused one or more partitions to carry massive amounts of data**.\n",
    "    \n",
    "    In fact:\n",
    "    \n",
    "    - One task processed **1.1 GiB of shuffle read** (vs 6.7 MiB median).\n",
    "    - It took **3.1 minutes**, while most tasks finished in <1â€¯s.\n",
    "    \n",
    "    Thatâ€™s a textbook example of **partition-level skew**.\n",
    "    \n",
    "- When does this happen most often?\n",
    "    - During **joins**, when one side has **skewed key distribution**.\n",
    "    - Especially when doing **SortMergeJoin**, because data has to be repartitioned and sorted by key.\n",
    "    - If the join key (e.g., `cust_id`) has a few very **\"hot\" values**, they dominate the shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ed5b5",
   "metadata": {},
   "source": [
    "## 2. Broadcast Join Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733962e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024)  # 10 MB\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")                    # Disable AQE\n",
    "df_joined = df_transactions.join(F.broadcast(df_customers), on=\"cust_id\", how=\"inner\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save(\"output/broadcast_join\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e633c",
   "metadata": {},
   "source": [
    " Details\n",
    "== Physical Plan ==\n",
    "OverwriteByExpression (10)\n",
    "+- * Project (9)\n",
    "   +- * BroadcastHashJoin Inner BuildRight (8)\n",
    "      :- * Filter (3)\n",
    "      :  +- * ColumnarToRow (2)\n",
    "      :     +- Scan parquet  (1)\n",
    "      +- BroadcastExchange (7)\n",
    "         +- * Filter (6)\n",
    "            +- * ColumnarToRow (5)\n",
    "               +- Scan parquet  (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49f827",
   "metadata": {},
   "source": [
    "## 3. AQE-Enabled Join Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c9e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024)  # Enable broadcast threshold\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")                     # Enable AQE\n",
    "df_joined = df_transactions.join(df_customers, on=\"cust_id\", how=\"inner\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save(\"output/aqe_join\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df1bbf",
   "metadata": {},
   "source": [
    "== Physical Plan ==\n",
    "OverwriteByExpression (18)\n",
    "+- AdaptiveSparkPlan (17)\n",
    "   +- == Final Plan ==\n",
    "      ResultQueryStage (11), Statistics(sizeInBytes=8.0 EiB)\n",
    "      +- * Project (10)\n",
    "         +- * BroadcastHashJoin Inner BuildRight (9)\n",
    "            :- * Filter (3)\n",
    "            :  +- * ColumnarToRow (2)\n",
    "            :     +- Scan parquet  (1)\n",
    "            +- BroadcastQueryStage (8), Statistics(sizeInBytes=32.2 MiB, rowCount=5.00E+3)\n",
    "               +- BroadcastExchange (7)\n",
    "                  +- * Filter (6)\n",
    "                     +- * ColumnarToRow (5)\n",
    "                        +- Scan parquet  (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06508814",
   "metadata": {},
   "source": [
    "| **Metric** | **AQE Join** |\n",
    "| --- | --- |\n",
    "| **Join Type Used** | BroadcastHashJoin (picked by AQE) |\n",
    "| **Execution Time** | 33 seconds |\n",
    "| **Input Data Size (Left Table)** | 862.7 MiB |\n",
    "| **Input Rows (Left Table)** | 39,790,092 |\n",
    "| **Input Data Size (Right Table)** | 14.99 KiB (post-filter), broadcast size 32.2 MiB |\n",
    "| **Broadcast Build Time** | 73 ms |\n",
    "| **Broadcast Collect Time** | 1.6 s |\n",
    "| **Time to Broadcast** | 41 ms |\n",
    "| **Shuffle Read / Write** | None (Broadcast avoids shuffle) |\n",
    "| **Join Output Rows** | 39,790,092 |\n",
    "| **Spill** | No memory or disk spill |\n",
    "| **Skew Handling** | Not needed â€” AQE chose optimal path |\n",
    "\n",
    "- **AQE (Adaptive Query Execution)** smartly **picked Broadcast Join** because:\n",
    "    - `df_customers` (right table) is only **32.2 MiB**, ideal for broadcast.\n",
    "    - Avoids shuffle, thus avoiding skew and spill entirely.\n",
    "    - Executes much faster than manual Sort-Merge Join (5.1â€“5.6 min).\n",
    "- **Execution Time**: **33s** â€” fastest among all 3 join methods.\n",
    "- **Why AQE is powerful**:\n",
    "    - Inspects runtime statistics (e.g. actual size of `df_customers` after filter).\n",
    "    - Dynamically selects the **best join strategy** based on actual data (not estimates).\n",
    "    - Helps **reduce skew**, optimize **partitioning**, and eliminate **unnecessary shuffles**.\n",
    "\n",
    "\n",
    "| Metric | Sort Merge Join | Manual Broadcast Join | AQE Join (Auto) |\n",
    "| --- | --- | --- | --- |\n",
    "| Join Type | SortMergeJoin | BroadcastHashJoin | BroadcastHashJoin |\n",
    "| Shuffle Read | 2.4 GiB | 0 | 0 |\n",
    "| Shuffle Write | 2.4 GiB | 0 | 0 |\n",
    "| Broadcast Size | N/A | 32.2 MiB | 32.2 MiB |\n",
    "| Execution Time | ~5.1â€“5.6 min | 1.1 min | **33s** |\n",
    "| Spill (Memory / Disk) | 512 MiB / 140 MiB | 0 / 0 | 0 / 0 |\n",
    "| Max Task Skew | 1.1 GiB read | None | None |\n",
    "| Output Rows | 39,790,092 | 39,790,092 | 39,790,092 |\n",
    "| Strategy Adapted at Runtime | âŒ | âŒ | âœ… |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e85d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
