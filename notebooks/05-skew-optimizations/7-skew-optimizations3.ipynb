{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4625ff3",
   "metadata": {},
   "source": [
    "This skew optimization is performed on a different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a90527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "# Set environment variables (local paths)\n",
    "os.environ[\"JAVA_HOME\"] = \"D:/Programs/Java\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"D:/Programs/hadoop\"\n",
    "os.environ[\"SPARK_HOME\"] = \"D:/Programs/spark/spark-3.5.6-bin-hadoop3\"  # Adjust if different\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"D:/Programs/spark/spark-3.5.6-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ec2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_trips_data(spark: SparkSession) -> DataFrame:\n",
    "    pu_loc_to_change = [\n",
    "        236, 132, 161, 186, 142, 141, 48, 239, 170, 162, 230, 163, 79, 234, 263, 140, 238, 107, 68, 138, 229, 249,\n",
    "        237, 164, 90, 43, 100, 246, 231, 262, 113, 233, 143, 137, 114, 264, 148, 151\n",
    "    ]\n",
    "\n",
    "    res_df = spark.read\\\n",
    "        .parquet(\"data/trips/*.parquet\")\\\n",
    "        .withColumn(\n",
    "            \"PULocationID\",\n",
    "            F.when(F.col(\"PULocationID\").isin(pu_loc_to_change), F.lit(237))\n",
    "            .otherwise(F.col(\"PULocationID\"))\n",
    "        )\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_on_skewed_data(spark: SparkSession):\n",
    "    trips_data = prepare_trips_data(spark=spark)\n",
    "    location_details_data = spark.read.option(\"header\", True).csv(\"data/taxi+_zone_lookup.csv\")\n",
    "\n",
    "    trips_with_pickup_location_details = trips_data\\\n",
    "        .join(location_details_data, F.col(\"PULocationID\") == F.col(\"LocationID\"), \"inner\")\n",
    "\n",
    "    # .groupBy(\"Zone\") \\\n",
    "\n",
    "    trips_with_pickup_location_details \\\n",
    "        .groupBy(\"Borough\") \\\n",
    "        .agg(F.avg(\"trip_distance\").alias(\"avg_trip_distance\")) \\\n",
    "        .sort(F.col(\"avg_trip_distance\").desc()) \\\n",
    "        .show(truncate=False, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|Borough      |avg_trip_distance |\n",
      "+-------------+------------------+\n",
      "|Brooklyn     |54.084378566790875|\n",
      "|Bronx        |50.5792505718606  |\n",
      "|Queens       |14.573507646515498|\n",
      "|Staten Island|11.273981415296637|\n",
      "|Unknown      |7.245918024787724 |\n",
      "|Manhattan    |5.424279480643658 |\n",
      "|EWR          |0.8988827712778211|\n",
      "+-------------+------------------+\n",
      "\n",
      "Elapsed_time: 24.47364377975464 seconds\n"
     ]
    }
   ],
   "source": [
    "# aqe disabled\n",
    "def create_spark_session_with_aqe_disabled() -> SparkSession:\n",
    "    conf = SparkConf() \\\n",
    "        .set(\"spark.driver.memory\", \"4G\") \\\n",
    "        .set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"201\") \\\n",
    "        .set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "    spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[8]\")\\\n",
    "        .config(conf=conf)\\\n",
    "        .appName(\"Read from JDBC tutorial\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark_session\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    spark = create_spark_session_with_aqe_disabled()\n",
    "\n",
    "    join_on_skewed_data(spark=spark)\n",
    "\n",
    "    print(f\"Elapsed_time: {(time.time() - start_time)} seconds\")\n",
    "   # time.sleep(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ac9cb",
   "metadata": {},
   "source": [
    "just 24secs with only borough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70c941",
   "metadata": {},
   "source": [
    "takes 35 secs with aqe disabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d5f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|Borough      |avg_trip_distance |\n",
      "+-------------+------------------+\n",
      "|Brooklyn     |54.084378566790875|\n",
      "|Bronx        |50.5792505718606  |\n",
      "|Queens       |14.573507646515498|\n",
      "|Staten Island|11.273981415296637|\n",
      "|Unknown      |7.245918024787724 |\n",
      "|Manhattan    |5.424279480644278 |\n",
      "|EWR          |0.8988827712778211|\n",
      "+-------------+------------------+\n",
      "\n",
      "Elapsed_time: 15.890237808227539 seconds\n"
     ]
    }
   ],
   "source": [
    "# aqe skew join enabled\n",
    "\n",
    "def create_spark_session_with_aqe_skew_join_enabled() -> SparkSession:\n",
    "    conf = SparkConf() \\\n",
    "        .set(\"spark.driver.memory\", \"4G\") \\\n",
    "        .set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"201\") \\\n",
    "        .set(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\") \\\n",
    "        .set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "        .set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"3\") \\\n",
    "        .set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256K\")\n",
    "\n",
    "    spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[8]\")\\\n",
    "        .config(conf=conf)\\\n",
    "        .appName(\"Read from JDBC tutorial\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark_session\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    spark = create_spark_session_with_aqe_skew_join_enabled()\n",
    "\n",
    "    join_on_skewed_data(spark=spark)\n",
    "\n",
    "    print(f\"Elapsed_time: {(time.time() - start_time)} seconds\")\n",
    "   # time.sleep(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69cab2",
   "metadata": {},
   "source": [
    "15 secs with only borough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497eddf1",
   "metadata": {},
   "source": [
    "Takes 25 secs to perform join operation with aqe enabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76321530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|Borough      |avg_trip_distance |\n",
      "+-------------+------------------+\n",
      "|Brooklyn     |54.08437856679085 |\n",
      "|Bronx        |50.57925057186062 |\n",
      "|Queens       |14.573507646515482|\n",
      "|Staten Island|11.27398141529664 |\n",
      "|Unknown      |7.245918024787716 |\n",
      "|Manhattan    |5.424279480643807 |\n",
      "|EWR          |0.8988827712778201|\n",
      "+-------------+------------------+\n",
      "\n",
      "âœ… The join and aggregation took: 17.64 seconds\n"
     ]
    }
   ],
   "source": [
    "def join_with_bucketing(spark: SparkSession):\n",
    "    \"\"\"Performs a join using bucketed tables and measures execution time.\"\"\"\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    num_buckets = 10\n",
    "\n",
    "    # Prepare and write data (we won't time this part)\n",
    "    trips_data = prepare_trips_data(spark=spark)\n",
    "    trips_data.write.bucketBy(num_buckets, \"PULocationID\").sortBy(\"PULocationID\").mode(\"overwrite\").saveAsTable(\"bucketed_trips_table\")\n",
    "\n",
    "    location_details_data = spark.read.option(\"header\", True).csv(\"data/taxi+_zone_lookup.csv\")\n",
    "    location_details_data.write.bucketBy(num_buckets, \"LocationID\").sortBy(\"LocationID\").mode(\"overwrite\").saveAsTable(\"bucketed_locations_table\")\n",
    "\n",
    "    # Read from the bucketed tables\n",
    "    bucketed_trips = spark.table(\"bucketed_trips_table\")\n",
    "    bucketed_locations = spark.table(\"bucketed_locations_table\")\n",
    "\n",
    "    # --- Start Timer ---\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Perform the join on the bucketed tables\n",
    "    trips_with_pickup_location_details = bucketed_trips.join(\n",
    "        bucketed_locations,\n",
    "        bucketed_trips[\"PULocationID\"] == bucketed_locations[\"LocationID\"],\n",
    "        \"inner\"\n",
    "    )\n",
    "\n",
    "    # Trigger action to evaluate the join and aggregations\n",
    "    # Using .collect() or .show() will trigger the computation\n",
    "    result_df = trips_with_pickup_location_details \\\n",
    "        .groupBy(\"Borough\") \\\n",
    "        .agg(F.avg(\"trip_distance\").alias(\"avg_trip_distance\")) \\\n",
    "        .sort(F.col(\"avg_trip_distance\").desc())\n",
    "\n",
    "    result_df.show(truncate=False, n=1000) # This action triggers the job\n",
    "\n",
    "    # --- Stop Timer ---\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"âœ… The join and aggregation took: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# (SparkSession initialization and function call would go here)\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BucketingTimingExample\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    join_with_bucketing(spark=spark)\n",
    "\n",
    "    #spark.stop()\n",
    "\n",
    "    time.sleep(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530d25a",
   "metadata": {},
   "source": [
    "Took 17.64 settings for performing join on PULocationID (Bucketing )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374976ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing join on salted keys...\n",
      "+-------------+------------------+\n",
      "|Borough      |avg_trip_distance |\n",
      "+-------------+------------------+\n",
      "|Brooklyn     |54.08437856679081 |\n",
      "|Bronx        |50.57925057186058 |\n",
      "|Queens       |14.573507646515468|\n",
      "|Staten Island|11.273981415296642|\n",
      "|Unknown      |7.245918024787745 |\n",
      "|Manhattan    |5.424279480643289 |\n",
      "|EWR          |0.8988827712778152|\n",
      "+-------------+------------------+\n",
      "\n",
      "Total execution time: 8.13 seconds\n"
     ]
    }
   ],
   "source": [
    "# salting\n",
    "def join_with_salting(spark: SparkSession):\n",
    "    \"\"\"Performs a join using the salting technique to handle skew and tracks execution time.\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- 1. Define Skewed Key and Salt Range ---\n",
    "    SKEWED_KEY = 237\n",
    "    SALT_RANGE = 10\n",
    "\n",
    "    # --- 2. Prepare the DataFrames ---\n",
    "    trips_data = prepare_trips_data(spark=spark)\n",
    "    location_details_data = spark.read.option(\"header\", True).csv(\"data/taxi+_zone_lookup.csv\")\n",
    "\n",
    "    # --- 3. Salt the Large (skewed) DataFrame ---\n",
    "    salted_trips_data = trips_data.withColumn(\n",
    "        \"salted_PULocationID\",\n",
    "        F.when(\n",
    "            F.col(\"PULocationID\") == SKEWED_KEY,\n",
    "            F.concat(F.col(\"PULocationID\"), F.lit(\"_\"), (F.rand() * SALT_RANGE).cast(\"int\"))\n",
    "        ).otherwise(\n",
    "            F.concat(F.col(\"PULocationID\"), F.lit(\"_\"), F.lit(0))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- 4. Explode the Small DataFrame ---\n",
    "    salt_array = F.array([F.lit(i) for i in range(SALT_RANGE)])\n",
    "\n",
    "    exploded_locations_data = location_details_data.withColumn(\n",
    "        \"salt\",\n",
    "        F.when(F.col(\"LocationID\") == SKEWED_KEY, salt_array).otherwise(F.array(F.lit(0)))\n",
    "    ).withColumn(\"salt\", F.explode(\"salt\"))\n",
    "\n",
    "    exploded_locations_data = exploded_locations_data.withColumn(\n",
    "        \"salted_LocationID\",\n",
    "        F.concat(F.col(\"LocationID\"), F.lit(\"_\"), F.col(\"salt\"))\n",
    "    )\n",
    "\n",
    "    # --- 5. Perform the Join on the Salted Keys ---\n",
    "    print(\"Performing join on salted keys...\")\n",
    "    trips_with_pickup_location_details = salted_trips_data.join(\n",
    "        exploded_locations_data,\n",
    "        salted_trips_data[\"salted_PULocationID\"] == exploded_locations_data[\"salted_LocationID\"],\n",
    "        \"inner\"\n",
    "    )\n",
    "\n",
    "    # --- 6. Aggregations ---\n",
    "    # trips_with_pickup_location_details \\\n",
    "    #     .groupBy(\"Zone\") \\\n",
    "    #     .agg(F.avg(\"trip_distance\").alias(\"avg_trip_distance\")) \\\n",
    "    #     .sort(F.col(\"avg_trip_distance\").desc()) \\\n",
    "    #     .show(truncate=False, n=1000)\n",
    "\n",
    "    trips_with_pickup_location_details \\\n",
    "        .groupBy(\"Borough\") \\\n",
    "        .agg(F.avg(\"trip_distance\").alias(\"avg_trip_distance\")) \\\n",
    "        .sort(F.col(\"avg_trip_distance\").desc()) \\\n",
    "        .show(truncate=False, n=1000)\n",
    "\n",
    "    # --- 7. End Time ---\n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName(\"SaltingExample\").getOrCreate()\n",
    "    join_with_salting(spark=spark)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340695c",
   "metadata": {},
   "source": [
    "took 8.13 secs just for borough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74582c",
   "metadata": {},
   "source": [
    "took approx 35.96 secs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
