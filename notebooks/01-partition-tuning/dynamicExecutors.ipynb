{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6698d188",
   "metadata": {},
   "source": [
    "### **Importing necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321499d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable displaying output of all expressions in a single Jupyter cell (not just the last one)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Suppress warnings to keep the output clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Required to manually set environment variables for Spark, Java, and Hadoop in a local Windows setup\n",
    "import os\n",
    "# Set the Java installation path\n",
    "os.environ[\"JAVA_HOME\"] = \"D:/Programs/Java\"  # Change this path based on your Java installation\n",
    "# Set the Hadoop home directory (required even if not using full Hadoop setup, for Windows compatibility)\n",
    "os.environ[\"HADOOP_HOME\"] = \"D:/Programs/hadoop\"  # Change if you installed Hadoop elsewhere\n",
    "# Set the Spark installation directory\n",
    "os.environ[\"SPARK_HOME\"] = \"D:/Programs/spark/spark-3.5.6-bin-hadoop3\"  # Match your Spark installation path\n",
    "\n",
    "# Use findspark to make PySpark (Spark Python API) importable\n",
    "import findspark\n",
    "findspark.init(\"D:/Programs/spark/spark-3.5.6-bin-hadoop3\")  # Initialize Spark environment using the specified path\n",
    "\n",
    "# Import specific PySpark modules for data processing and transformations\n",
    "from pyspark.sql import SparkSession              # Main entry point for DataFrame and SQL functionality in Spark\n",
    "import pyspark.sql.functions as F                 # Import Spark SQL functions with alias 'F'\n",
    "from pyspark.sql.types import IntegerType         # For defining schema types\n",
    "from pyspark.sql.types import DoubleType          # For schema definition using double precision (float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74f7f3",
   "metadata": {},
   "source": [
    "### **Setting Up Multiple Executors Using Spark's Standalone Cluster**\n",
    "\n",
    "Follow the steps below to manually start a Spark master and multiple workers on your local machine using the Spark Standalone Cluster mode.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Step 1**: Open Spark `bin` Directory in Command Prompt\n",
    "\n",
    "```bash\n",
    "cd D:\\Programs\\spark\\spark-3.5.6-bin-hadoop3\\bin\n",
    "```\n",
    "\n",
    "> Make sure to run all the following commands from within this directory.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Step 2**: Start the Spark Master Node\n",
    "\n",
    "```bash\n",
    "spark-class2.cmd org.apache.spark.deploy.master.Master\n",
    "```\n",
    "\n",
    "* After running the above command, visit the Spark Master Web UI at:\n",
    "  [http://localhost:8080/](http://localhost:8080/)\n",
    "\n",
    "* Copy the master's URL from the UI — it will look something like:\n",
    "\n",
    "```text\n",
    "spark://192.168.171.138:7077\n",
    "```\n",
    "\n",
    "This URL is needed to connect worker nodes to the master.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Step 3**: Start Worker Nodes (Executors)\n",
    "\n",
    "Each worker must be launched from a **separate command prompt window**. For every executor you want, repeat the following step with different resource configurations:\n",
    "\n",
    "```bash\n",
    "spark-class2.cmd org.apache.spark.deploy.worker.Worker -c <num_cores> -m <memory> spark://192.168.171.138:7077\n",
    "```\n",
    "\n",
    "Example commands:\n",
    "\n",
    "```bash\n",
    "spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 1 -m 1G spark://192.168.171.138:7077\n",
    "spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 2 -m 2G spark://192.168.171.138:7077\n",
    "spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 1 -m 2G spark://192.168.171.138:7077\n",
    "spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 2 -m 1G spark://192.168.171.138:7077\n",
    "```\n",
    "\n",
    "* `-c` specifies the number of CPU cores assigned to the executor.\n",
    "* `-m` specifies the amount of memory assigned to the executor.\n",
    "\n",
    "> The workers will register with the master, and you can monitor them at [http://localhost:8080/](http://localhost:8080/)\n",
    "\n",
    "---\n",
    "\n",
    "##### Spark Standalone Cluster is Now Ready\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5b8c2",
   "metadata": {},
   "source": [
    "### **Setting up spark session - Dynamic Allocation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca193012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in .master add IP of master\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Dynamic Allocation\")\n",
    "    .master(\"spark://192.168.171.138:7077\")\n",
    "    .config(\"spark.executor.cores\", 1)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", 0)\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 3)\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", 1)\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db78dac0",
   "metadata": {},
   "source": [
    "```python\n",
    "    .appName(\"Dynamic Allocation\")  # Sets the name of your Spark application\n",
    "```\n",
    "* `SparkSession.builder`: Starts the configuration of your Spark session.\n",
    "* `.appName(\"Dynamic Allocation\")`: Names your application. Useful for identifying it in Spark UI (`http://localhost:4040` or the master web UI).\n",
    "\n",
    "\n",
    "```python\n",
    "    .master(\"spark://192.168.171.138:7077\")  # Connect to the master node of your standalone Spark cluster\n",
    "```\n",
    "* The master URL should be the same one shown on your Spark master UI (`spark://<master-ip>:7077`).\n",
    "* It tells the Spark driver where the cluster manager (master) is running.\n",
    "\n",
    "\n",
    "```python\n",
    "    .config(\"spark.executor.cores\", 1)  # Each executor will use 1 CPU core\n",
    "    .config(\"spark.executor.memory\", \"512M\")  # Each executor will be allocated 512MB memory\n",
    "```\n",
    "* These are the base resources given to each executor.\n",
    "* Keep these values small if you're running locally or testing on a low-resource machine.\n",
    "\n",
    "\n",
    "```python\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)  # Enables dynamic allocation of executors\n",
    "```\n",
    "* **Dynamic Allocation** lets Spark scale the number of executors up or down based on workload.\n",
    "\n",
    "\n",
    "```python\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", 0)  # Minimum number of executors\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 3)  # Maximum number of executors\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", 1)  # Number of executors to start with\n",
    "```\n",
    "* Spark will begin with 1 executor and scale up to a maximum of 3 depending on the workload.\n",
    "* If there are idle resources or fewer tasks, Spark may scale down to 0.\n",
    "\n",
    "\n",
    "```python\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)  # Enables tracking of shuffle dependencies\n",
    "```\n",
    "* **Shuffle Tracking** helps Spark track shuffles and decide when it’s safe to remove executors.\n",
    "* This makes dynamic allocation more effective **even without external shuffle service**, which is not commonly set up in standalone/local mode.\n",
    "\n",
    "\n",
    "```python\n",
    "spark  # Shows basic info about the created SparkSession (useful in Jupyter or REPL)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### **Summary**:\n",
    "\n",
    "* You're connecting to a standalone Spark cluster running at `spark://192.168.171.138:7077`.\n",
    "* You're enabling **dynamic allocation** to allow Spark to automatically scale executors based on the workload.\n",
    "* Executors will be created with 1 core and 512MB memory, and their number will range from 0 to 3 based on the tasks.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24678e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be3bce",
   "metadata": {},
   "source": [
    "### **Execute the Job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the transactions_file path wherever your data resides\n",
    "transactions_file = \"D:/Internship/spark-project-main/pyspark_test_project/data/transactions.parquet\"\n",
    "df_transactions = spark.read.parquet(transactions_file)\n",
    "df_transformed = (\n",
    "    df_transactions\n",
    "    .withColumn(\"amt\", F.col(\"amt\").cast(DoubleType()))\n",
    "    .filter(F.col(\"amt\") > 10)\n",
    "    .groupBy(\"city\")\n",
    "    .agg(F.avg(\"amt\").alias(\"avg_amt\"))\n",
    ")\n",
    "# change the output directory whereever you have to store the results\n",
    "df_transformed.write.mode(\"overwrite\").csv(f\"output/repartition_4\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
