{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6698d188",
   "metadata": {},
   "source": [
    "### **Importing necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321499d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for running pyspark in jupyter notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "# Manually set environment variables (local paths)\n",
    "os.environ[\"JAVA_HOME\"] = \"D:/Programs/Java\" # Adjust if different\n",
    "os.environ[\"HADOOP_HOME\"] = \"D:/Programs/hadoop\" # Adjust if different\n",
    "os.environ[\"SPARK_HOME\"] = \"D:/Programs/spark/spark-3.5.6-bin-hadoop3\"  # Adjust if different\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"D:/Programs/spark/spark-3.5.6-bin-hadoop3\") # Initialize findspark manually\n",
    "\n",
    "# Modules for running pyspark\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74f7f3",
   "metadata": {},
   "source": [
    "### **Setting up multiple executors using Spark's Standalone cluster**\n",
    "Run the following commands in command prompt to start master and worker nodes\n",
    "\n",
    "1) Open command prompt in the bin directory of spark \\\n",
    "   `cd D:\\Programs\\spark\\spark-3.5.6-bin-hadoop3\\bin` \\\n",
    "\n",
    "2) Creating master \\\n",
    "    `spark-class2.cmd org.apache.spark.deploy.master.Master` \\\n",
    "    Visit `http://localhost:8080/` to see your master \\\n",
    "    Copy master's IP address from there `spark://192.168.171.138:7077` \\\n",
    "    \n",
    "3) Now create workers. Open the same bin directory in command prompt. For each executor you have to open separate command  prompt. Now create worker and connent them to master by adding the same IP as master \\\n",
    "    -c 1: means 1 core (specify the number accordingly) \\\n",
    "    -m 1G: means 1GB memory (specify the number accordingly) \\\n",
    "    `spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 1 -m 1G spark://192.168.171.138:7077` \\\n",
    "    `spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 2 -m 2G spark://192.168.171.138:7077` \\\n",
    "    `spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 1 -m 2G spark://192.168.171.138:7077` \\\n",
    "    `spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 2 -m 1G spark://192.168.171.138:7077`\n",
    "\n",
    "Now your have created multiple executors, you can now set up spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5b8c2",
   "metadata": {},
   "source": [
    "### **Setting up spark session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca193012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in .master add IP of master\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VSCodeSparkSession\") \\\n",
    "    .master(\"spark://192.168.171.138:7077\") \\\n",
    "    .getOrCreate()\\\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24678e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be3bce",
   "metadata": {},
   "source": [
    "### **Execute the Job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the transactions_file path wherever your data resides\n",
    "transactions_file = \"D:/Internship/spark-project-main/pyspark_test_project/data/transactions.parquet\"\n",
    "df_transactions = spark.read.parquet(transactions_file)\n",
    "df_transformed = (\n",
    "    df_transactions\n",
    "    .withColumn(\"amt\", F.col(\"amt\").cast(DoubleType()))\n",
    "    .filter(F.col(\"amt\") > 10)\n",
    "    .groupBy(\"city\")\n",
    "    .agg(F.avg(\"amt\").alias(\"avg_amt\"))\n",
    ")\n",
    "# change the output directory whereever you have to store the results\n",
    "df_transformed.write.mode(\"overwrite\").csv(f\"output/repartition_4\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
